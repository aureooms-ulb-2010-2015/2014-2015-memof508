\section{Algorithms}
\label{tree:sorting:alg}

According to our model (\ref{tree:sorting:model}) we only consider comparison-based sorting algorithms.


\begin{table}
	\begin{center}
	\caption{Comparison sorts complexity}
	\label{tree:sorting:alg/table}
	\begin{tabular}{|c|c|c|c|c|}

	\hline
	Name & Best\tablefootnote{\label{tree:sorting:alg/table!bestisn} The reason some of those algorithms have a best case of \BigO{n} is that they make use of already ordered subsequences, e.g. for an already ordered sequence only a linear check is required. This gives us some clue that will be expanded in \ref{tree:supi}.} & Average & Worst & Source\\\hline\hline
	Quicksort & $n \log n$ & $1.39~n \log n$ & $n^2$ & \cite{hoare1962quicksort}\\\hline
	Dual Pivot Quicksort & $n \log n$ & $1.32~n \log n$ & $n^2$ & \cite{yaroslavskiy2009dual}\\\hline
	Merge sort & $n \log n$ & $1~n \log n$ & $n \log n$ & \begin{tabular}{@{}l@{}} \cite{leiserson2001introduction} \\ \cite{goldstine1948planning} \end{tabular} \\\hline
	Heapsort & $n \log n$ & $1~n \log n$ & $n \log n$ & \cite{leiserson2001introduction}\\\hline
	Introsort & $n \log n$ & $1~n \log n$ & $n \log n$ & \cite{musser1997introspective}\\\hline
	Timsort & $n$ & $1~n \log n$ & $n \log n$ & \cite{martelli2006python}\\\hline
	Smoothsort & $n$ & $1~n \log n$ & $n \log n$ & \cite{dijkstra1982smoothsort}\\\hline
	Shellsort & $n$ & — & — & \cite{shell1959high}\\\hline

	\end{tabular}
	\end{center}
\end{table}


In \ref{tree:sorting:alg/table} you can see a list of algorithms for which the average case execution complexity matches the Stirling's approximation of the \concept{ITLB} up to a $\BigO{1}$ factor.

Considering practical use of these algorithms, even though quicksort has a much higher multiplicative coefficient it is still a good candidate due to it's simplicity and cache locality of it's operations.

The average complexity of shellsort is not known for the moment, depending on the implementation one can achieve a complexity of eiter $\BigO{n \log^2 n}$ or $\BigO{n^{\frac{3}{2}}}$, the main changing parameter of different implementations being a sequence of integer called a \emph{gap sequence}. An open problem is to find whether or not there exist a gap sequence that makes the algorithm run in $\BigO{n \log n}$.

In addition to algorithms found in \ref{tree:sorting:alg/table}, we may also mention that any data structure allowing to retrieve inserted elements following a total order with insertion and retrieve operations in $\BigO{\log n}$ can be used to build an algorithm whose complexity is $\BigO{n \log n}$. Examples are,

\begin{itemize}
\item Binary tree sort which uses a \emph{binary tree};
\item Tournament sort which uses a \emph{priority queue};
\item Patience sorting which uses a \emph{van Emde Boas tree}.
\end{itemize}

For materials on those data structure see \cite{sleator1985self}, \cite{leiserson2001introduction} and \cite{van1975preserving}.