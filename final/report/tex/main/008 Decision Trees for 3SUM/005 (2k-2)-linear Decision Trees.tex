\section{$(2k-2)$-linear Decision Trees}

In \cite{gronlund:2014}, \citeauthor{gronlund:2014} build a decision tree
solving the \threeSUM problem using
a subquadratic number of linear queries. Moreover, two algorithms
solving the \threeSUM problem in subquadratic time are demonstrated.

Those results refute a long-lived conjecture stating that the \threeSUM problem
can not be solved in subquadratic time on a \emph{Random Access Machine}.

In the classical quadratic algorithm for \threeSUM, $3$-linear queries of the
form \emph{do these $3$ numbers sum up to $0$} are used. In their first
algorithm, breaking the \BigOmega{n^2} bound on the number of queries,
\citeauthor{gronlund:2014} use $4$-linear queries.

We list hereunder the steps of an algorithm deciding the one-set
version of \threeSUM with input set $\A$, $\card{\A} = n$. By looking at the
number of queries this algorithm requires to finish, we will prove the
decision tree complexity of \threeSUM is \BigO{n^{\sfrac{3}{2}}
\sqrt{\log n}}.

The algorithm goes as follows,

\begin{algorithm}

\item[1.] Sort $\A$ in increasing order as $\A(1) < \ldots < \A(n)$.

\item[2.] Partition $\A$ into $\ceil{\sfrac{n}{g}}$ groups $\A_1, \ldots,
\A_{\ceil{\sfrac{n}{g}}}$ of size at most $g$, where $\A_i \bydef \enum{\A(1 + (i-1)g),
\ldots, \A(ig)}$ and $\A_{\ceil{\sfrac{n}{g}}}$ may contain less than $g$ elements.
The first and last elements of $\A_i$ are $\min(A_i) = \A(1 + (i-1)g)$ and
$\max(\A_i) = \A(ig)$.

\item[3.] Sort $\D \bydef \bigcup\limits_{i = 1}^{\ceil{\sfrac{n}{g}}}
\group{\A_i-\A_i} = \enum{a - a' \st a, a' \in \A_i}$.

\item[4.] For all $i,j \in \enum{1, \ldots \ceil{\sfrac{n}{g}}}$, sort $\A_{i,j}
\bydef \A_i + \A_j = \enum{a + b \st a \in \A_i \text{ and } b \in \A_j}$.

\item[5.] For $k$ from 1 to $n$,

\item[5.1.] Initialize $\lo \gets 1$ and $\hi \gets \ceil{\sfrac{k}{g}}$ to be
the group index of $\A(k)$.

\item[5.2.] Repeat until $\hi < \lo$:

\item[5.2.1.] If $-\A(k) \in \A_{\lo,\hi}$, report ``solution found'' and halt.

\item[5.2.2.] If $\max(\A_{\lo}) + \min(\A_{\hi}) > -\A(k)$ then decrement
$\hi$, otherwise increment $\lo$.

\item[6.] Report ``no solution'' and halt.

\end{algorithm}

The algorithm is arranged in two phases. The first phase, steps \step{1}
through \step{4}, is a preprocessing phase where we build a data structure
for efficient lookups in preparation of the second phase. The second phase, steps
\step{5} through \step{6}, is constitued of a loop where we will search
for all $a, b \le A(k)$ such that $a + b + A(k) = 0$ using the efficient lookup
data structure built during the first phase.

Step \step{1} requires \BigO{n \log n} queries using any optimal sorting
algorithm. Step \step{2} explicits
the indexing convention we use througout the algorithm and requires no query.
We will skip the explanation of step \step{3} for the moment. If done as is,
step \step{4} would require to sort $(\frac{n}{g})^2$ $\A_i + \A_j$ sets,
each of which, by \ref{theorem:fredman:1976}, would require \BigO{g^2} queries
to sort. In total, step \step{4} would need \BigO{n^2} queries to be
executed. Clearly this way of doing would not qualify as a subquadratic
algorithm. We will come back at it later after analyzing the complexity of the
second phase.

The rest of the algorithm is straightforward. The loop of step \step{5} is
\BigO{n}. The loop of step \step{5.2} is \BigO{\frac{n}{g}} since we
always either increment $\lo$ or decrement $\hi$. The lookup at step
\step{5.2.1} can be achieved with $\log g^2 = \BigO{\log g}$ queries using
a standard binary search procedure. This gives us a total complexity of
\BigO{\frac{n^2}{g} \log g} for the second phase.

The catch in \cite{gronlund:2014} is that they show it is possible to
achieve step \step{4} in a subquadratic number of queries. The idea of this
magic trick is to make use of a simple property referred to as ``Fredman's
trick''. The property to observe is that $a + b < c + d \iff a - c < d - b$.

The magic trick is materialized as step \step{3}, by computing the sorted
order on $\D$, we can execute step \step{4} without making any additional
query since if $a_i, a_i' \in \A_i \text{ and } a_j, a_j' \in \A_j, a_i + a_j <
a_i' + a_j' \iff a_i - a_i' < a_j' - a_j$. And this time, it can be
subquadratic! Using \ref{theorem:fredman:1976} again, the total number of
queries of step \step{3} is $\log \card{\Gamma} + 2N$. In this case $N =
(\frac{n}{g}) g^2 = gn$ and $\card{\Gamma} \le ((\frac{n}{g})^2 (g^2)^2)^{2n} =
(n^2 g^2)^{2n} \le (n^4)^{2n} = n^{8n}$, hence the number of queries asked for
step \step{3} is \BigO{n \log n + gn}. We say that step \step{3} is a
comparison efficient way of accomplish step \step{4}.

\citet*{gronlund:2014} generalize this result further by reducing\footnote{see
\ref{tree:3sum:kldt}} \kLDT to an unbalanced \threeSUM problem, and apply a
three-set version of their decision tree to solve it, proving the following
theorem,

\begin{theorem}
When $k \ge 3$ is odd, there is a $(2k-2)$-linear decision tree for \kLDT with
depth \BigO{n^{\sfrac{k}{2}} \sqrt{\log n}}.
\end{theorem}

Finally, for the case of $k = 3$, they provide two subquadratic \threeSUM
algorithms. A deterministic one running in \BigO{n^2/(\log n / \log \log
n)^{\sfrac{2}{3}}} time and a randomized one running in \BigO{n^2 (\log \log
n)^2 / \log n} time with high probability, the first one refuting the \threeSUM
conjecture.
