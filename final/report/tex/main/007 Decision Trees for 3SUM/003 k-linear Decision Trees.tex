\section{$k$-linear Decision Trees}

In \citet*{erickson:1999}, it is shown that we cannot solve \threeSUM in
subquadratic time using only the $3$-linear decision tree as a model. A
general proof for the \kLDT problem in the $k$-linear decision tree model
is featured. In the case of $k$
even, a bound of \BigOmega{n^{\frac{k}{2}}} is demonstrated. For odd $k$, the
bound is \BigOmega{n^{\frac{k+1}{2}}}.

Their result is stated as follows,
\begin{theorem}
The optimal depth of a \(k\)-linear decision tree that solves
a \(k\)-variable linear satisfiability problem is \BigTheta{n^{\ceil{k/2}}}.
\end{theorem}

The proof uses an adversary argument which can be explained geometrically. As
we will see later, we can solve \(k\)-variable linear satisfiability problems
by modeling them as point location problems in arrangement of hyperplanes. The
goal of a point location problem in an arrangement of hyperplanes is to
determine which cell of the arrangement contains the input point. The adversary
argument of \citet*{erickson:1999} is that there exists a cell having
\BigOmega{n^{\ceil{k/2}}} boundary facets and thus if the input point is inside
that cell, then an algorithm must check if the point is on any of the facets of
that cell. If the algorithm stops without doing so, an adversary could freely
move the point from the inside of the cell to its border, changing the output
without the algorithm noticing it.

For information, we hereunder detail the classical \BigO{n^2} algorithm for
\threeSUM, successfully attaining the lower bound demonstrated in
\cite{erickson:1999} for $3$-linear decision trees. In this description we
handle the three-set version of \threeSUM, \ie the case where $a$, $b$ and $c$
are taken from the three sets $\A$, $\B$ and $\C$ instead of a single set $\U$.
The description comes from \citet*{gronlund:2014}.


\begin{algorithm}
\item[1.] Sort $\A$ and $\B$ in increasing order as $\A_1 < \ldots <
\A_{\card{\A}}$ and $\B_1 < \ldots < \B_{\card{\B}}$
\item[2.] For each $c \in \C$,
\item[2.1.] Initialize $\lo \gets 1$ and $\hi \gets \card{\B}$
\item[2.2.] Repeat until $\lo > \card{\A}$ or $\hi < 1$:
\item[2.2.1.] If $\A_{\lo} + \B_{\hi} + c = 0$, report witness $(\A_{\lo},
\B_{\hi}, c)$
\item[2.2.2.] If $\A_{\lo} + \B_{\hi} + c > 0$ then decrement $\hi$, otherwise
increment $\lo$.
\end{algorithm}


The running time complexity of this algorithm is
\BigO{\card{\C}(\card{\A}+\card{\B})}, hence for the case where
$\A = \B = \C$, \ie the one-set version of \threeSUM, its complexity is
\BigO{n^2}.

At the end of \cite{erickson:1999}, it is asked whether other kinds of
decision trees would prove to be more powerful. The next section cites a
progress in this direction made by \citet*{ailon:2005}.
