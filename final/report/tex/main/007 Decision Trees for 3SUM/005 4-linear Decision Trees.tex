\section{\(4\)-linear Decision Trees}

\citet*{gronlund:2014} build a decision tree
solving the \threeSUM problem using a subquadratic number of linear queries.
Moreover, two algorithms solving the \threeSUM problem in subquadratic time on
a Random Access Machine are
given.
These results refute the long-lived conjecture stating that \threeSUM
cannot be solved in subquadratic time on a \emph{Random Access Machine}.

In the classical quadratic algorithm for \threeSUM, $3$-linear queries of the
form \emph{do these $3$ numbers sum up to $0$} are used. In their first
algorithm, breaking the \BigOmega{n^2} bound on the number of queries,
\citeauthor{gronlund:2014} use $4$-linear queries.
We list hereunder the steps of this algorithm. The algorithm decides the one-set
version of \threeSUM with input set $\A$, $\card{\A} = n$. By looking at the
number of queries this algorithm requires to finish, we prove the
decision tree complexity of \threeSUM is \BigO{n^{\sfrac{3}{2}}
\sqrt{\log n}}.
\begin{algorithm}[\citet*{gronlund:2014}]
\item[1.] Sort $\A$ in increasing order as $\A(1) < \ldots < \A(n)$.

\item[2.] Partition $\A$ into $\ceil{\sfrac{n}{g}}$ groups $\A_1, \ldots,
\A_{\ceil{\sfrac{n}{g}}}$ of size at most $g$, where $\A_i \bydef \enum{\A(1 + (i-1)g),
\ldots, \A(ig)}$ and $\A_{\ceil{\sfrac{n}{g}}}$ may contain less than $g$ elements.
The first and last elements of $\A_i$ are $\min(\A_i) = \A(1 + (i-1)g)$ and
$\max(\A_i) = \A(ig)$.

\item[3.] Sort $\D \bydef \bigcup\limits_{i = 1}^{\ceil{\sfrac{n}{g}}}
\group{\A_i-\A_i} = \enum{a - a' \st a, a' \in \A_i}$.

\item[4.] For all $i,j \in \enum{1, \ldots \ceil{\sfrac{n}{g}}}$, sort $\A_{i,j}
\bydef \A_i + \A_j = \enum{a + b \st a \in \A_i \text{ and } b \in \A_j}$.

\item[5.] For $k$ from 1 to $n$,

\item[5.1.] Initialize $\lo \gets 1$ and $\hi \gets \ceil{\sfrac{k}{g}}$ to be
the group index of $\A(k)$.

\item[5.2.] Repeat until $\hi < \lo$:

\item[5.2.1.] If $-\A(k) \in \A_{\lo,\hi}$, report ``solution found'' and halt.

\item[5.2.2.] If $\max(\A_{\lo}) + \min(\A_{\hi}) > -\A(k)$ then decrement
$\hi$, otherwise increment $\lo$.

\item[6.] Report ``no solution'' and halt.
\end{algorithm}

The algorithm is arranged in two phases. In the first phase, steps \step{1}
through \step{4}, we build a data structure
for efficient lookups in preparation of the second phase. The second phase, steps
\step{5} through \step{6}, consists of a loop where we search
for all $a, b \le \A(k)$ such that $a + b + \A(k) = 0$ using the efficient lookup
data structure built during the first phase.

Step \step{1} requires \BigO{n \log n} queries using any optimal sorting
algorithm. Step \step{2} explains
the indexing convention we use throughout the algorithm and requires no query.
We skip the explanation of step \step{3} for the moment. If done as is,
step \step{4} would require to sort $(\frac{n}{g})^2$ $\A_i + \A_j$ sets,
each of which, by \ref{theorem:fredman:1976}, would require \BigO{g^2} queries
to sort. In total, step \step{4} would need \BigO{n^2} queries to be
executed. Clearly this way of doing would not qualify as a subquadratic
algorithm. We will come back at it later after analyzing the complexity of the
second phase.

The rest of the algorithm is straightforward. The loop of step \step{5} is
\BigO{n}. The loop of step \step{5.2} is \BigO{\frac{n}{g}} since we
always either increment $\lo$ or decrement $\hi$. The lookup at step
\step{5.2.1} can be achieved with $\log g^2 = \BigO{\log g}$ queries using
a standard binary search procedure. This gives us a total complexity of
\BigO{\frac{n^2}{g} \log g} for the second phase.

The catch in \citet*{gronlund:2014} is that they show it is possible to
achieve step \step{4} in a subquadratic number of queries. The idea is to use a simple property referred to as ``Fredman's
trick''. The property is that $a + b < c + d \iff a - c < d - b$.

The trick is materialized as step \step{3}, by computing the sorted
order on $\D$, we can execute step \step{4} without making any additional
query since if $a_i, a_i' \in \A_i \text{ and } a_j, a_j' \in \A_j, a_i + a_j <
a_i' + a_j' \iff a_i - a_i' < a_j' - a_j$. And this time, it can be
subquadratic. Applying the results of \ref{theorem:fredman:1976} again, the
total number of queries used during step \step{3} is $\log \card{\Gamma} + 2N$,
where \(\Gamma\) is the set of cells of an arrangement of hyperplanes of the
kind \( a_i - a_i' + a_j - a_j' = 0 \) in \(\R^{2n}\).
In this case $N = (\frac{n}{g}) g^2 = gn$ and $\card{\Gamma} \le
((\frac{n}{g})^2 (g^2)^2)^{2n} = (n^2 g^2)^{2n} \le (n^4)^{2n} = n^{8n}$.
Hence, the number of queries used during step \step{3} is \BigO{n \log n + gn}.
We say that step \step{3} is a comparison efficient way of accomplishing step
\step{4}.

\citet*{gronlund:2014} generalize this result further by reducing \kLDT to an
unbalanced \threeSUM problem, and apply a three-set version of their decision
tree to solve it. The details of this reduction technique are explained in
the next section.

Finally, for the case of $k = 3$, they provide two subquadratic \threeSUM
algorithms. A deterministic one running in \BigO{n^2/(\log n / \log \log
n)^{\sfrac{2}{3}}} time and a randomized one running in \BigO{n^2 (\log \log
n)^2 / \log n} time with high probability, the first one refuting the \threeSUM
conjecture. We do no detail those here, our focus being the query complexity.
