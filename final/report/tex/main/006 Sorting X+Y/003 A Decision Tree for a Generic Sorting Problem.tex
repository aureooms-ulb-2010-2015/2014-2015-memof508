\section{A Decision Tree for a Generic Sorting Problem}
\label{tree:xy:dt}

In this section we expose an algorithm due to \citet*{fredman:1976}
that solves sorting problems making at most $\log\card{\Gamma} + 2n$ queries,
where $\Gamma$ is the set of possible linear extensions of the input. In the
next section we show how this algorithm can be used in the context of the
sorting \XY problem.

\citet{fredman:1976} defines a generic sorting problem as follows,

\begin{problem}
Let $\S = \enum{x_1, \ldots, x_n}$ be an $n$-element set. We define a sorting
problem on these element to be a pair $(\Gamma, P)$, where $\Gamma$ is a
subset of the $n!$ possible linear orderings on $\S$, and
$P$ is a family $\family{\A_1, \ldots, \A_r}$ of disjoint nonempty sets that
partition $\Gamma$, so that $\Gamma = \A_1 \cup \ldots \cup \A_r$. Given an
(unknown) ordering $\omega \in \Gamma$, we want to determine to which set $\A_j$ the
ordering $\omega$ belongs by performing a sequence of comparisons between
pairs of elements, $x_i \ask{\le} x_j$. A comparison algorithm is said to
solve $(\Gamma, P)$ if the following condition is satisfied. Upon representing
the algorithm as a comparison tree $T$, we associate with each leaf $L$ of $T$
the subset $\Gamma_L$ consisting of the orderings in $\Gamma$ that are
consistent with the
path through $T$ ending at $L$. For each $L$ we must have $\Gamma_L \subseteq
\A_j$ for some $\A_j \in P$.
\end{problem}

As usual, we are interested in the decision tree complexity of this
problem, \ie for fixed \(\Gamma\) and \(P\) the minimum height of a decision
tree allowing to find \(\A_j\) for any of the possible inputs \(\omega\) compatible
with \(\Gamma\). We denote this complexity by $N(\Gamma, P)$.

Since the final goal is to sort \XY, we are only interested in the case
where $P$ is a partition of $\Gamma$ into singleton sets, as we desire to
compute the correct linear extension of \XY out of all its feasible linear
extensions. As $P$ is the same for every $\Gamma$ in this case, we
always write $N(\Gamma)$ to mean $N(\Gamma, P)$.

\citet{fredman:1976} proves the following theorem,

\begin{theorem}[Fredman 1976 \cite{fredman:1976}]\label{theorem:fredman:1976}
A poset of cardinality $n$, whose linear extension is known to be contained in
$\Gamma$ can be sorted with $N(\Gamma) = \log \card{\Gamma} + 2n$ pairwise
comparisons.
\end{theorem}

For our use case, the $n$ of the theorem is the $n^2$ elements of the set
\XY. In the previous section we already found a lower bound for the problem of
finding a linear extension of a \(n \times n\) lattice. We also showed that if
we only use the information contained in the poset structure of \XY, then
sorting \XY is the same problem as finding a linear extension of a \(n \times
n\) lattice. In the next section we compute the lower bound for the
sorting \XY problem when we remove the restrictions we considered earlier. This
new lower bound is our $\card{\Gamma}$ for the sorting \XY problem.

To prove the correctness of the theorem, \citet{fredman:1976} builds a biased
insertion sort algorithm that we now explain.

The algorithm we consider is an iterative algorithm running in \(n\)
iterations. Suppose that at iteration \(k\) of the algorithm we have already sorted a
subset of \(\S\) of size \(k-1\).

\begin{displaymath}
\underbrace{x_1, \ldots, x_{k-1}}_{\text{subset}}~:~\underbrace{x_{i_1} <
\ldots < x_{i_{k-1}}}_{\text{relative ordering}}
\end{displaymath}

What we do at iteration $k$ is insert $x_k$ at the appropriate location
in the already existing relative ordering to extend it to $x_{i_1} < \ldots <
x_{i_k}$.

We want to reformulate this insertion process using an equivalent formulation
that involves location numbers $b_1, \ldots, b_k$. The location
number $b_k$ means that at iteration $k$, $x_k$ is inserted at position
$b_k$ implying that $i_{b_k} = k$.

The location number $b_k$ is defined formally as $b_k = \card{\enum{x_j \st j
\le k, x_j \le x_k}}$, \ie the number of elements from the relative ordering
of iteration $k$ that are smaller or equal to $k$. Since $x_k \le x_k$ is
always true, $b_k$ is at least $1$ and since $\card{\enum{x_j \st j \le k}} =
k$, $b_k$ is at most $k$. We thus have,
$$ 1 \le b_k \le k$$
and we say $b_k$ is the location number of $x_k$ after it is inserted into
$x_{i_1} < \ldots < x_{i_{k-1}}$.

We have established a correspondence between orderings on sets of cardinality
$n$ and $n$-tuples $(b_1, \ldots, b_n)$ with $1 \le b_i \le i$. Hence, $\Gamma$
can be represented as a set of $n$-tuples of $b_k$ and inserting $x_k$ is
tantamount to the determination of $b_k$. In other words, asking for $x_k
\ask{\le} x_{i_J}$ is equivalent to asking $b_k \ask{\le} J$.

To prove the theorem, \citet{fredman:1976} builds an algorithm whose existence
implies an even more general result,

\begin{lemma}
Given $n \ge 1$, let $\S$ be a finite set of $n$-tuples with unrestricted
positive integer coordinates. Given an unknown $n$-tuple in $\S$, we can
determine its components, $b_1, b_2, \ldots$, in that order, by making queries
of the form \(b_i \ask{\le} J\), and with a total of no more than $\log \card{\S}
+ 2n$ such queries.
\end{lemma}

We show how to implement an algorithm matching the above description. First, in
order to simplify the notation, we describe a recursive version of our
algorithm. The $\nth{k}$ recursion step of the recursive algorithm processes
the $\nth{k}$ iteration. A complete execution of the algorithm would determine
the $n$-tuple $(b_1, \ldots, b_n)$.  At the beginning of iteration $k$, we
already have determined a prefix $(b_1, \ldots, b_{k-1})$ of this $n$-tuple.
What remains to be computed is the suffix $(b_k, \ldots, b_n)$ of the
$n$-tuple. In the recursive version of the algorithm we forget about the
already computed prefix since it cannot change and instead of writing $(b_k,
\ldots, b_n)$ we write $(b'_1, b'_2, \ldots)$ where $b'_1 = b_k$, $b'_2 =
b_{k+1}$, \etc. From here on till the end of this section we write $b_i$ to
mean $b'_i$ for the sake of simplicity.

We now list the steps of the recursive algorithm,
\begin{algorithm}

\item[Input] $\S$, a set of $n$-tuples with unrestricted positive integer
coordinates.

\item[1.] Let $\S(k) \bydef \card{\enum{(b_1, \ldots) \in \S \st b_1 = k}}$,
since $\S$ is a finite set, there are finitely many values $i_1 < \ldots < i_l$
such that $\S(i_j) > 0$. We map each $i_j$ to $j$
and we write $N_j = \S(i_j)$. Using this mapping we can, without loss of
generality, assume \(i_j = j\).

\item[2.] Partition the interval $(0,1]$ into $l$ adjacent intervals each of
size $\frac{N_j}{\card{S}}$. For each $r \in (0,1]$, we define $0 \le J(q) \le
l$ to be the number of interval midpoints lying to the left of $r$.

\item[3.] Search and find $b_1 = i$ by applying binary search to the partitioned
interval using queries of the form \(b_1 \ask{\le} J(q)\).

\item[Induction] If $n > 1$, we call the procedure again with\\
$\enum{(b_2, \ldots, b_n) \st (b_1, b_2, \ldots, b_n) \in \S \text{ and } b_1 =
i}$ as the input set.

\end{algorithm}

The goal of step \step{1} is to offer a mapping between outputs of $J(q)$ and
the possible original values for $b_1$. After the substitution of $i_j$ with
$j$, asking $b_1 \ask{\le} J(q)$ means asking $b_1 \ask{\le} i_{J(q)}$
before the substitution takes place. Hence, without loss of generality we can
now assume \(i_j = j\). The value $N_j$ expresses the frequency or ``number of
occurrences'' of $b_1 = j$ in the set $\S$, therefore
\begin{displaymath}
\sum\limits_{j=1}^{l} N_j = \card{\S}.
\end{displaymath}

In step \step{2} we partition $(0,1]$ in a way that allows binary search to
be efficient with respect to the complexity of the remaining induction steps.
The partitioning procedure assigns interval sizes to possible values for
$b_1$ proportional to their frequency in $\S$. Hence, if the interval
associated with $b_1 = i$ is large, we have to make a small number of
queries to find it but the cardinality of $\S$ at the next induction step
stays large. Otherwise, if the same interval is small, the number of queries
required to find it is large but in this case, the induction step
handles a small $\S$ set.

The only step where we make queries is \step{3}. We use binary
search as follows. First we ask if $b_1$ lies in the first or the second
half of $(0,1]$, by asking $b_1 \ask{\le} J(\frac{1}{2})$, and then we continue
this procedure with the interval designated by the answer to our question as
our query interval. If the designated interval is the first half of $(0,1]$,
\ie $(0, \frac{1}{2}]$, because $b_1 \le J(\frac{1}{2})$ the next question we
ask is $b_1 \ask{\le} J(\frac{1}{4})$. Otherwise the designated interval
is $(\frac{1}{2},1]$ and the next question $b_1 \ask{\le} J(\frac{3}{4})$,
\etc. We stop when we have ascertained that the query interval we are left with
does not contain any of the original interval midpoints of our partitioned
$(0,1]$ interval.

We now prove that if we have to perform $r$ queries before finding such an
interval, $b_1 = J(q)$ for all $q$ in this interval, then $N_{b_1} \le
\frac{4\card{\S}}{2^r}$. Suppose that $b_1 = i$ and that the length of the
$\nth{i}$ interval, containing the $\nth{i}$ midpoint, is
$\frac{N_i}{\card{\S}} > \frac{4}{2^r}$, thus has radius $> \frac{2}{2^r} =
\frac{1}{2^{r-1}}$.

Imagine we have already performed $r-2$ queries to search for $b_1$ without
successfully ending the search, we now analyze what happens after the
$\nth{(r-1)}$ query is made. After having made $r-1$ queries, our current query
interval has length $\sfrac{1}{2^{r-1}}$. This query interval must be one of
all the possible $2^{r-1}$ intervals of the partition of $(0,1]$ in adjacent
intervals of length $\sfrac{1}{2^{r-1}}$ starting at $0$. We now prove a rather
simple lemma,

\begin{lemma}
\label{lemma:xy:dt:qi}
All query intervals we consider can be written $(\frac{c}{2^{r-1}},
\frac{c+1}{2^{r-1}}]$ for some integer $c$ with $0 \le c \le 2^{r-1}$.
Moreover, for all such intervals containing the \(\nth{i}\) midpoint.
\begin{displaymath}
J\mleft(\frac{c}{2^{r-1}}\mright) < i \le J\mleft(\frac{c+1}{2^{r-1}}\mright)
\end{displaymath}
\end{lemma}

\begin{proof}
At the beginning of the binary search algorithm, the search interval is
$(0,1]$, it excludes \(0\) because \(J(0) = 0\) while \(b_1 \ge 1\). We now
assume the current query interval is \((L,R]\). If during the search a
question \(b \ask{\le} J(q)\) gets a ``yes'' answer then we update the upper
bound and get a new query interval \((L,q]\). Otherwise, \(b > J(q)\) and we
can update the lower bound and the query interval becomes \((q,R]\). In both
cases the lower bound remains excluded. At the beginning of the algorithm,
\(r - 1 = 0\), \(L=0\) and \(R=1\), hence \(c = 0\). Now for any query
interval $(\frac{c}{2^{r-1}}, \frac{c+1}{2^{r-1}}]$ the query point is \(q =
\frac{\sfrac{(c + (c + 1))}{2}}{2^{r-1}} = \frac{2c + 1}{2^r}\). Depending on the
outcome of the query \(b \ask{\le} J(q)\) the new query interval is
either \((\frac{2c}{2^{r}}, \frac{2c+1}{2^{r}}]\) or \((\frac{2c+1}{2^{r}},
\frac{2c+2}{2^{r}}]\). \ref{lemma:xy:dt:qi} follows by induction.
\end{proof}

To be valid, the current query interval must contain the \(\nth{i}\)
midpoint. Now, if this query interval contained more than the \(\nth{i}\)
midpoint, the algorithm would have to use at least one additional query.
Since the radius of the \(\nth{i}\) interval is \(> \sfrac{1}{2^{r-1}}\) it is
not possible for the query interval, which has radius \( \frac{c+1-c}{2^{r-1}}
 = \frac{1}{2^{r-1}}\) to contain the \(\nth{i}\) midpoint and at
the same time escape the \(\nth{i}\) interval. Hence, the \(\nth{i}\) midpoint
is the only midpoint inside the query interval. We conclude that,
\begin{displaymath}
i-1 = J\mleft(\frac{c}{2^{r-1}}\mright) < b_1 \le J\mleft(\frac{c+1}{2^{r-1}}\mright) = i
\end{displaymath}
thus $b_1 = i$ and the search ends before performing a $\nth{r}$ query.

We have proved that if the search for $b_1$ requires at least $r$ queries then
we can be sure that $N_{b_1} \le \frac{4\card{\S}}{2^r}$. This means that at
the next recursion step, $\card{\S_{k+1}} \le \frac{4\card{\S_k}}{2^r}$. If
$T(\card{\S}, n)$ denotes the number of comparisons required to find the
correct $n$-tuple out of set $\S$ with our recursive algorithm, then,
$$T(\card{\S}, n) \le r + T(\min\mleft(\frac{4\card{\S}}{2^r}, \card{\S}\mright), n-1)$$
and
$$T(\card{\S}, 1) = \BigO{\log \card{\S}}$$

Clearly, we can only divide $\card{\S}$ by $2$ $\log \card{\S}$ times and since
we have a multiplicative factor of $4$ in the recurrence, we can always ask $2$
\emph{useless} queries at each step that do not decrease $\card{\S}$ for the
next iteration. Hence,

\begin{displaymath}
T(\card{\S}, n) \le \log \card{\S} + 2n
\end{displaymath}

\nb{\citet*{moran:2015} give an average-case analysis of this algorithm when the
distribution over the inputs is not uniform.}

