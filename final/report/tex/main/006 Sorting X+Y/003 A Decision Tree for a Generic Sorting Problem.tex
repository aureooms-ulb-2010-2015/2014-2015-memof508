\section{A Decision Tree for a Generic Sorting Problem}
\label{tree:xy:dt}

We expose an algorithm due to \citet*{fredman:1976}
that solves sorting problems making at most $\log\card{\Gamma} + 2n$ queries,
where $\Gamma$ is the set of possible linear extensions of the input. In the
next section we show how this algorithm can be used in the context of
Sorting \XY.

\citet{fredman:1976} defines a generic sorting problem as follows
\begin{problem}[Generic sorting problem]
Let $\S = \enum{x_1, \ldots, x_n}$ be an $n$-element set. We define a sorting
problem on these element to be a pair $(\Gamma, P)$, where $\Gamma$ is a
subset of the $n!$ possible linear orderings on $\S$, and
$P$ is a family $\family{\A_1, \ldots, \A_r}$ of disjoint nonempty sets that
partition $\Gamma$, so that $\Gamma = \A_1 \cup \ldots \cup \A_r$. Given an
(unknown) ordering $\omega \in \Gamma$, we want to determine to which set $\A_j$ the
ordering $\omega$ belongs by performing a sequence of comparisons between
pairs of elements, $x_i \ask{\le} x_j$. A comparison algorithm is said to
solve $(\Gamma, P)$ if the following condition is satisfied. Upon representing
the algorithm as a comparison tree $T$, we associate with each leaf $L$ of $T$
the subset $\Gamma_L$ consisting of the orderings in $\Gamma$ that are
consistent with the
path through $T$ ending at $L$. For each $L$ we must have $\Gamma_L \subseteq
\A_j$ for some $\A_j \in P$.
\end{problem}

As usual, we are interested in the decision tree complexity of this
problem, \ie for fixed \(\Gamma\) and \(P\) the minimum height of a decision
tree allowing to find \(\A_j\) for any of the possible inputs \(\omega\) compatible
with \(\Gamma\). We denote this complexity by $N(\Gamma, P)$.

Since the final goal is to sort \XY, we are only interested in the case
where $P$ is a partition of $\Gamma$ into singleton sets, as we desire to
compute the correct linear extension of \XY out of all its feasible linear
extensions. Since $P$ is the same for every $\Gamma$ in this case, we
always write $N(\Gamma)$ to mean $N(\Gamma, P)$.

\citet{fredman:1976} proves the following theorem
\begin{theorem}[\citet*{fredman:1976}]\label{theorem:fredman:1976}
A poset of cardinality $n$, whose linear extension is known to be contained in
$\Gamma$ can be sorted with $N(\Gamma) = \log \card{\Gamma} + 2n$ pairwise
comparisons.
\end{theorem}
For our use case, \(n\) is the cardinality of the set
\XY. In the previous section we already found a lower bound for the problem of
finding a linear extension of a \(n \times n\) lattice. We also showed that if
we only use the information contained in the poset structure of \XY, then
sorting \XY is the same as finding a linear extension of a \(n \times
n\) lattice. In the next section we compute the actual information-theoretic lower bound for
Sorting \XY. This
new lower bound is our $\card{\Gamma}$ in \ref{theorem:fredman:1976}.

To prove the correctness of the theorem, \citet{fredman:1976} builds a biased
insertion sort algorithm that we explain in the next paragraphs.

The algorithm we consider is an iterative algorithm running in \(n\)
iterations. Suppose that at iteration \(k\) of the algorithm we have already sorted a
subset of \(\S\) of size \(k-1\).
\begin{displaymath}
\underbrace{x_1, \ldots, x_{k-1}}_{\text{subset}}~:~\underbrace{x_{i_1} <
\ldots < x_{i_{k-1}}}_{\text{relative ordering}}
\end{displaymath}

What we do at iteration $k$ is insert $x_k$ at the appropriate location
in the already existing relative ordering to extend it to $x_{i_1} < \ldots <
x_{i_k}$.

We want to reformulate this insertion process using an equivalent formulation
that involves location numbers $b_1, \ldots, b_k$. The location
number $b_k$ means that at iteration $k$, $x_k$ is inserted at position
$b_k$ implying that $i_{b_k} = k$.

The location number $b_k$ is defined formally as $b_k = \card{\enum{x_j \st j
\le k, x_j \le x_k}}$, \ie the number of elements from the relative ordering
of iteration $k$ that are smaller or equal to $k$. Since $x_k \le x_k$ is
always true, $b_k$ is at least $1$ and since $\card{\enum{x_j \st j \le k}} =
k$, $b_k$ is at most $k$. We thus have,
\begin{displaymath}
1 \le b_k \le k.
\end{displaymath}
We say $b_k$ is the location number of $x_k$ after its insertion into
$x_{i_1} < \cdots < x_{i_{k-1}}$.

We have established a correspondence between orderings on sets of cardinality
$n$ and $n$-tuples $(b_1, \ldots, b_n)$ with $1 \le b_i \le i$. Hence, $\Gamma$
can be represented as a set of $n$-tuples of $b_k$ and inserting $x_k$ is
tantamount to the determination of $b_k$. In other words, asking whether $x_k
\ask{\le} x_{i_j}$ is equivalent to asking whether $b_k \ask{\le} j$.

To prove the theorem, \citet{fredman:1976} gives an algorithm whose existence
implies an even more general result.
\begin{lemma}[\citet*{fredman:1976}]
Given $n \ge 1$, let $\S$ be a finite set of $n$-tuples with unrestricted
positive integer coordinates. Given an unknown $n$-tuple \(b\) in $\S$, we can
determine its components, \(b_1, b_2, \ldots\), in that order, by making queries
of the form \(b_i \ask{\le} j, j \in \N\), and with a total of no more than $\log \card{\S}
+ 2n$ such queries.
\end{lemma}
We show how to implement an algorithm matching the above description. First, in
order to simplify the notation, we describe a recursive version of our
algorithm. The $\nth{k}$ recursion step of the recursive algorithm processes
the $\nth{k}$ iteration. A complete execution of the algorithm would determine
the $n$-tuple $(b_1, \ldots, b_n)$.  At the beginning of iteration $k$, we
already have determined a prefix $(b_1, \ldots, b_{k-1})$ of this $n$-tuple.
What remains to be computed is the suffix $(b_k, \ldots, b_n)$ of the
$n$-tuple. In the recursive version of the algorithm we forget about the
already computed prefix since it cannot change and instead of writing $(b_k,
\ldots, b_n)$ we write $(b'_1, b'_2, \ldots)$ where $b'_1 = b_k$, $b'_2 =
b_{k+1}$, \etc. From here on till the end of this section we write $b_i$ to
mean $b'_i$ for the sake of simplicity.

We list the steps of the recursive algorithm
\begin{algorithm}[\citet*{fredman:1976}]

\item[input] $\S$, a set of $n$-tuples with unrestricted positive integer
coordinates.

\item[1.] Let $\S(k) \bydef \card{\enum{(b_1, \ldots) \in \S \st b_1 = k}}$,
since $\S$ is a finite set, there are finitely many values $i_1 < \ldots < i_l$
such that $\S(i_j) > 0$. We map each $i_j$ to $j$
and we write $N_j = \S(i_j)$. Using this mapping we can, without loss of
generality, assume \(i_j = j\).

\item[2.] Partition the interval $(0,1]$ into $l$ adjacent intervals each of
size $\frac{N_j}{\card{S}}$. For each $r \in (0,1]$, we define $0 \le J(q) \le
l$ to be the number of interval midpoints lying to the left of $r$.

\item[3.] Search and find $b_1 = i$ by applying binary search to the partitioned
interval using queries of the form \(b_1 \ask{\le} J(q)\).

\item[induction] If $n > 1$, we call the procedure again with\\
$\enum{(b_2, \ldots, b_n) \st (b_1, b_2, \ldots, b_n) \in \S \text{ and } b_1 =
i}$ as the input set.

\end{algorithm}

The goal of step \step{1} is to offer a mapping between outputs of $J(q)$ and
the possible original values for $b_1$. After the substitution of $i_j$ with
$j$, asking $b_1 \ask{\le} J(q)$ means asking $b_1 \ask{\le} i_{J(q)}$
before the substitution takes place. Hence, without loss of generality we can
assume \(i_j = j\). The value $N_j$ expresses the frequency or \emph{number of
occurrences} of $b_1 = j$ in the set $\S$, therefore
\begin{displaymath}
\sum_{j=1}^{l} N_j = \card{\S}.
\end{displaymath}

In step \step{2} we partition $(0,1]$ in a way that allows binary search to
be efficient with respect to the complexity of the remaining induction steps.
The partitioning procedure assigns interval sizes to possible values for
$b_1$ proportional to their frequency in $\S$. Hence, if the interval
associated with $b_1 = i$ is large, we have to make a small number of
queries to find it but the cardinality of $\S$ at the next induction step
remains large. Otherwise, if the same interval is small, the number of queries
required to find it is large but in this case, the induction step
handles a smaller set \(\S\).

The only step where we make queries is \step{3}. We use binary
search as follows. First we ask if $b_1$ lies in the first or the second
half of $(0,1]$, by asking $b_1 \ask{\le} J(\frac{1}{2})$, and then we continue
this procedure with the interval designated by the answer to our question as
our query interval. If the designated interval is the first half of $(0,1]$,
\ie $(0, \frac{1}{2}]$, because $b_1 \le J(\frac{1}{2})$ the next question we
ask is $b_1 \ask{\le} J(\frac{1}{4})$. Otherwise the designated interval
is $(\frac{1}{2},1]$ and the next question $b_1 \ask{\le} J(\frac{3}{4})$,
\etc. We stop when we have ascertained that the query interval we are left with
contains only one of the interval midpoints of our partitioned
$(0,1]$ interval.

We prove that if we have to perform $r$ queries before finding such an
interval, $b_1 = J(q)$ for all $q$ in this interval, then $N_{b_1} \le
\frac{4\card{\S}}{2^r}$. Suppose that $b_1 = i$ and that the length of the
$\nth{i}$ interval, containing the $\nth{i}$ midpoint, is
$\frac{N_i}{\card{\S}} > \frac{4}{2^r}$, thus has radius $> \frac{2}{2^r} =
\frac{1}{2^{r-1}}$.

After having made $r-1$ queries, our current query
interval has length $\sfrac{1}{2^{r-1}}$. This query interval must be one of
all the possible $2^{r-1}$ intervals of the partition of $(0,1]$ in adjacent
intervals of length $\sfrac{1}{2^{r-1}}$ starting at $0$. We prove a rather
simple lemma
\begin{lemma}
\label{lemma:xy:dt:qi}
All query intervals we consider can be written $(\frac{c}{2^{r-1}},
\frac{c+1}{2^{r-1}}]$ for some integer $c$ with $0 \le c \le 2^{r-1}$.
Moreover, for all such intervals containing the \(\nth{i}\) midpoint
\begin{displaymath}
J\bigg(\frac{c}{2^{r-1}}\bigg) < i \le J\bigg(\frac{c+1}{2^{r-1}}\bigg).
\end{displaymath}
\end{lemma}

\begin{proof}
At the beginning of the binary search algorithm, the search interval is
$(0,1]$. It excludes \(0\) because \(J(0) = 0\) while \(b_1 \ge 1\). Let us
assume the current query interval is \((L,R]\). If during the search a
question \(b \ask{\le} J(q)\) gets a ``yes'' answer then we update the upper
bound and get a new query interval \((L,q]\). Otherwise, \(b > J(q)\) and we
can update the lower bound and the query interval becomes \((q,R]\). In both
cases the lower bound remains excluded. At the beginning of the algorithm,
\(r - 1 = 0\), \(L=0\) and \(R=1\), hence \(c = 0\). For any query
interval $(\frac{c}{2^{r-1}}, \frac{c+1}{2^{r-1}}]$ the query point is \(q =
\frac{\sfrac{(c + (c + 1))}{2}}{2^{r-1}} = \frac{2c + 1}{2^r}\). Depending on the
outcome of the query \(b \ask{\le} J(q)\) the new query interval is
either \((\frac{2c}{2^{r}}, \frac{2c+1}{2^{r}}]\) or \((\frac{2c+1}{2^{r}},
\frac{2c+2}{2^{r}}]\). \ref{lemma:xy:dt:qi} follows by induction.
\end{proof}

To be valid, the current query interval must contain the \(\nth{i}\)
midpoint. If this query interval contained more than the \(\nth{i}\)
midpoint, the algorithm would have to use at least one additional query.
Since the radius of the \(\nth{i}\) interval is \(> \sfrac{1}{2^{r-1}}\) it is
not possible for the query interval, which has radius \( \frac{c+1-c}{2^{r-1}}
 = \frac{1}{2^{r-1}}\) to contain the \(\nth{i}\) midpoint and at
the same time escape the \(\nth{i}\) interval. Hence, the \(\nth{i}\) midpoint
is the only midpoint inside the query interval. We conclude that,
\begin{displaymath}
i-1 = J\bigg(\frac{c}{2^{r-1}}\bigg) < b_1 \le J\bigg(\frac{c+1}{2^{r-1}}\bigg) = i,
\end{displaymath}
thus $b_1 = i$ and the search ends before performing a $\nth{r}$ query.

We have proved that if the search for $b_1$ requires at least $r$ queries then
we can be sure that $N_{b_1} \le \frac{4\card{\S}}{2^r}$. Letting \(\S_{k}\)
denote the set of feasible solutions at the beginning of step \(k\) and
\(r_k\) denote the number of queries made at step \(k\), this means that at
step \(k+1\), \(\card{\S_{k+1}} \le \frac{4\card{\S_k}}{2^{r_k}}\).
After the last recursion step we have determined all components of
the \(n\)-tuple to be found and hence the size of the feasible solutions set
is \(1\). We have
\begin{displaymath}
1 \le \frac{4\card{\S_n}}{2^{r_n}} \le \frac{4^2\card{\S_{n-1}}}{2^{r_n +
r_{n-1}}} \le \ldots \le \frac{4^n\card{\S_1}}{2^{\sum_{i=1}^{n} r_i}},
\end{displaymath}
and since \(\S_1 = \S\) we get
\begin{displaymath}
\sum_{i=1}^{n} r_i \le \log (4^n \card{\S}) = \log \card{\S} + 2n.
\end{displaymath}

\citet*{moran:2015} give an average-case analysis of this algorithm when the
distribution over the inputs is not uniform.

