\section{Algorithms for the Sorting Problem}
\label{tree:sorting:alg}

We detail three algorithms that can be used to solve the sorting
problem efficiently. According to the model defined in \Cref{tree:sorting:model},
we only consider comparison-based sorting algorithms.

Before we start, let us make a quick observation. The sorting problem can
easily be solved using at most \(\binom{n}{2} = \BigO{n^2}\) queries. A first
way to achieve this bound is by iteratively computing and removing the
minimum\footnote{In the following paragraphs we use the words minimum,
smaller
and smallest as well as the symbol \(\le\). If one replaces minimum by maximum,
smaller by larger, smallest by largest and \(\le\) by \(\ge\) in those paragraphs,
their content remains valid.}
element from the input set. A second way is to iteratively
extend a sorted sequence by insertion of an element taken (and removed) from
the input set. \selectionsort and \bubblesort are two algorithms based on the
first method. \insertionsort is an algorithm based on the second method.
Because the complexity of these algorithms does not meet the \ITLB, we need
to think about more clever methods\footnote{Although it is possible to obtain
a \BigO{n \log n} comparisons algorithm by using binary search as the search
method of the \insertionsort algorithm, this approach does not lead directly to
a practical algorithm. Indeed, in the machine world, one must also take into
account other types of operations. In the case of \insertionsort,
the number of memory read and write operations in standard implementations is
quadratic on average.}.

\subsection{\quicksort}
The first efficient algorithm we explain is \quicksort \cite{hoare:1962}.
The idea behind the \quicksort algorithm is to divide the input set \(\S\),
\(\card{\S} = n\), into two disjoint subsets \(\L\) and \(\U\) that can be
sorted independently from one another. To enforce this independence condition,
the first part of the algorithm consists in computing \(\L\) and \(\U\) such
that \(l \le u, \Forall l \in \L, u \in U\). Once \(\L\) and \(\U\) are known,
the algorithm sorts these two sets recursively.
\begin{algorithm}[\quicksort]
\item[1.] If \(n < 2\) then \(\S\) is sorted and the algorithm stops.
\item[2.] Choose a pivot element \(p \in \S\) and compare it with all other
elements of \(\S\) by querying the oracle \(n - 1\) times.
\item[3.] Partition \(\T = \S \setminus \enum{p}\) into two disjoint subsets
\(\L = \enum{t \in \T \st t \le p}\) and \(\U = \T \setminus \L\).
\item[4.] Sort \(\L\).
\item[5.] Sort \(\U\).
\end{algorithm}

The \quicksort algorithm is a \emph{divide-and-conquer algorithm}: it splits a bigger
problem into several smaller problems whose solutions can be combined to build
the solution of the bigger problem. Ideally, \(p\) would be chosen to be the
median element of \(\S\) so that \(\card{\L} = \ceil{\frac{n-1}{2}}\) and
\(\card{\U} = \floor{\frac{n-1}{2}}\), because then the recursion depth of the
algorithm would be bounded by \(\floor{\log \card{S}}\) resulting in a
worst-case complexity of \BigO{n \log n}. However, if \(p\) can be any element
of \(\S\), then, in the worst case, \(p\) is the minimum (or maximum) element of
\(\S\), and \(\L\) (or \(\U\)) is empty. In this case, \quicksort behaves like
the \selectionsort algorithm and its worst-case complexity degrades to
\BigO{n^2}. It is possible \cite{blum:1973} to compute a good pivot or even the
median element in \BigO{n} comparisons. However, it was shown \cite{hoare:1962}
that selecting the pivot uniformly at random leads to an average case
complexity of \(1.39n \log n - \BigO{n}\) comparisons, and for real-world computers this
approach tends to be more efficient than the safe one.

\subsection{\mergesort}
A second efficient algorithm is \mergesort
\cite{goldstine:1948,leiserson:2001}. Like \quicksort, \mergesort is a
divide-and-conquer algorithm. While in \quicksort one makes the comparisons from
the partition step before solving the two subproblems, in \mergesort we
first solve two subproblems and then recombine them with a merge step that
requires \BigO{n} comparisons.
\begin{algorithm}[\mergesort]
\item[1.] If \(n < 2\) then \(\S\) is sorted and the algorithm stops.
\item[2.] Arbitrarily partition \(\S\) into two disjoint subsets \(\L\) and
\(\U\) such that \(\card{\L} = \ceil{\frac{n}{2}}\) and \(\card{\U} =
\floor{\frac{n}{2}}\).
\item[3.] Sort \(\L\).
\item[4.] Sort \(\U\).
\item[5.] Merge \(\L\) and \(\U\) by iteratively removing the smallest element
of \(\S\).
\end{algorithm}

Note that in step \step{5} there are at most \(n - 1\) iterations. Since \(\L\)
and \(\U\) are sorted, the smallest element of \(\S\) can be computed as the
smallest element between the smallest element of \(\L\) and the smallest
element of \(\U\). Hence, each iteration requires a single comparison.

Unlike the previous algorithm, we always obtain a balanced partition of
\(\S\) out of the partitioning step. Hence, the worst-case complexity of this
algorithm is \BigO{n \log n}. Another way to obtain this complexity measure
is to solve the following recurrence relation (\citet*{OEIS:A001855}):
\begin{displaymath}
M(n) = M(\ceil{\frac{n}{2}}) + M(\floor{\frac{n}{2}}) + n - 1 \le n \log n +
n + \BigO{\log n}.
\end{displaymath}

\subsection{\heapsort}
The last algorithm we explain is the \heapsort algorithm
\cite{williams:1964}. A \emph{heap} is a data structure that allows one to
efficiently lookup (\BigO{1}) or remove (\BigO{\log n}) the smallest element
from a set \(\S\). A heap can be seen as a complete binary tree where a node
is smaller or equal to its children. The last condition is called the heap property.

There exist two main procedures to update the contents of a heap.
The first one is \siftdown which consists in \emph{sifting} a node down
the tree until it is smaller or equal to all of its children. Below are the
steps of the \siftdown algorithm with input node \(N\)
\begin{algorithm}[\siftdown]
\item[1.] If \(N\) is a leaf of the tree the algorithm stops.
\item[2.] Compute \(C = \min \text{children}(N)\).
\item[3.] If \(N \le C\) the algorithm stops.
\item[4.] Otherwise, swap \(C\) and \(N\).
\item[5.] Sift down \(N\) again.
\end{algorithm}
The second one is \siftup. This operation is similar to \siftdown.
We \emph{sift} a node up the tree until it is larger or equal to its parent.
\begin{algorithm}[\siftup]
\item[1.] If \(N\) is the root of the tree the algorithm stops.
\item[2.] Compute \(P = \text{parent}(N)\).
\item[3.] If \(N \ge P\) the algorithm stops.
\item[4.] Otherwise, swap \(P\) and \(N\).
\item[5.] Sift up \(N\) again.
\end{algorithm}

For example, to remove the minimum from a binary heap one replaces the root
node with the rightmost leaf of the complete binary tree and then sifts down
the new root. To add a new node, one attaches this node to the tree to the
right of the rightmost leaf (or as the leftmost node of a new level
of the tree if the last level is complete) and then sifts up this new node.
Since a complete tree is balanced, sifting down a node in a binary heap uses
at most \(2 \ceil{\log n}\) comparisons while sifting up a node uses at most
\(\ceil{\log n}\)
comparisons.

However, it is possible to build a heap using \SmallO{n \log n}
comparisons. Assume we have a complete tree that does not respect the heap
property. This tree has \(\ceil{\frac{n}{2}}\) leaves, at most
\(\ceil{\frac{n}{4}}\) nodes of height \(1\), at most \(\ceil{\frac{n}{8}}\)
nodes of height \(2\), \dots, \ie at most \(\ceil{\frac{n}{2^{1+h}}}\) nodes
of height \(h\).
One applies \siftdown to each node of the tree right-to-left,
bottom-to-top, starting from the rightmost leaf and ending at the root. The
total number of comparisons is at most
\begin{displaymath}
2 \cdot ( 0 \cdot \left\lceil\frac{n}{2}\right\rceil + 1 \cdot
\left\lceil\frac{n}{4}\right\rceil + 2 \cdot
\left\lceil\frac{n}{8}\right\rceil + 3 \cdot \left\lceil\frac{n}{16}\right\rceil + \cdots ) = \BigO{n}.
\end{displaymath}

To sort \(\S\) using a heap, one builds a heap with \BigO{n} comparisons and
then iteratively extracts the minimum of this heap and restores the heap
property with \(2\ceil{\log (n-1)}\) comparisons. The maximum number of
comparisons involved in these extraction steps is
\begin{displaymath}
H(n) = 2 \sum_{i=2}^{n-1} \ceil{\log i} \le 2 ( \log (n-1)! + n - 2 ),
\end{displaymath}
which is approximately twice as much comparisons than what can be found by
solving the recurrence relation for the \mergesort algorithm.
Note that in the \heapsort algorithm we did not use the
\siftup operation. There exists \cite{wegener:1993} a variation of the
extracting step leading to an algorithm called \bottomupheapsort that uses at
most \(1.5~n \log n + \BigO{n}\) comparisons.

\subsection{Other Algorithms}
There exist other interesting ideas to solve the sorting problem. For
example: \shellsort \cite{shell:1959}, \smoothsort \cite{dijkstra:1982},
\introsort \cite{musser:1997} and \timsort \cite{martelli:2006}.

Considering practical uses of sorting algorithms, \quicksort is one of the most
efficient algorithms on real-world computers due to its simplicity and cache
locality of its operations, even though \quicksort has a higher
multiplicative factor than \mergesort regarding the number of
comparisons and even though \quicksort has a quadratic worst-case complexity. Standard libraries of the most
popular programming languages (\Java (up to \Java 6), \CC, \CXX) use their own
version of the \quicksort algorithm as their sorting procedure. An exception is
\Python which uses the \timsort algorithm since 2002. Note also that up to \Java
6, \Java used a \mergesort algorithm for sorting non-primitive types. The
reason behind this choice is that objects in \Java can be different while
having the same comparison ``key''. Since the \Java \mergesort algorithm is
implemented as a stable sorting procedure, it offers the users the guarantee
that the sorting algorithm does not reverse the \emph{input-order} of
\emph{equal-key} objects.
The classical \quicksort algorithm is not used in this case because it is not
stable.

Recently, there has been some interest in a dual-pivot \quicksort
implementation by \citet*{yaroslavskiy:2009}. It is not the first time this
approach is considered. Previous attempts \cite{sedgewick:1980} had concluded
that a dual-pivot \quicksort was worse than the classical one in practice.
Analysis of the version of \citet*{yaroslavskiy:2009} by \citet*{wild:2012}
reveals that this new version uses \(1.32n \log n - 2.46n + \BigO{\log n}\)
comparisons on average. This is less than the classical \quicksort algorithm which uses
\(1.39n \log n - 1.51n + \BigO{\log n}\) comparisons on average. However, in practice one must also take into account
other kinds of operations. For example, the number of element swaps in the
dual-pivot \quicksort of \citeauthor{yaroslavskiy:2009} is twice the number of
element swaps of the classic \quicksort by \citeauthor{hoare:1962}. This
algorithm is used as the sorting procedure of arrays of primitive type for
the \Java language (since \Java 7). Although it performs more
swaps \cite{wild:2012}, and uses more Java bytecodes \cite{wild:2013}, the
dual-pivot version of \citet*{yaroslavskiy:2009} outperformed the previous
\quicksort implementation experimentally. Note that the stable \mergesort
algorithm of \Java 7 was changed for a \timsort implementation, which is
also stable. One of the subtleties of the \timsort algorithm is that it tries
to exploit already sorted subsequences of the input. On a nearly sorted (or
reverse sorted) input the \timsort algorithms runs in \BigO{n} time.

A remark concerning \shellsort. The average time complexity of \shellsort is not
known. Indeed, there are multiple ways of implementing this
algorithm because of its definition. The \shellsort algorithm is configurable
with a sequence of integers called a \emph{gap sequence}. This gap sequence
parameter leads to different time complexities. For example, depending on the
implementation, one can achieve a complexity of either $\BigO{n \log^2 n}$ or
$\BigO{n^{\frac{3}{2}}}$. There are some practical variants of this algorithm
for which the time complexity is not known. An open problem is to find whether
there exists a gap sequence that makes the algorithm run in $\BigO{n \log
n}$.

In addition to classical sorting algorithms, we may also mention that any data
structure allowing to retrieve inserted elements in an ordered fashion
(according to some total order) with insertion and retrieve operations in
$\BigO{\log n}$ time complexity can be used to build an algorithm whose time
complexity is $\BigO{n \log n}$. Examples are
\binarytreesort which uses a \emph{binary tree} and
\tournamentsort which uses a \emph{priority queue}.
For example implementations of those data structures see
\citet*{sleator:1985} for the binary tree, and
\citet*{leiserson:2001} for the priority queue.


\subsection{Is the Sorting Problem a Solved Problem?}
Although there has been an enormous amount of research directed towards sorting
algorithms in the field of Computer Science, there still exists a linear term
gap between the most efficient algorithms with respect to the number of
comparisons and the \ITLB for the sorting problem.

Out of the algorithms we explained, \mergesort is one of the
sorting algorithms whose query complexity comes closest
to the \ITLB, \ie
\begin{displaymath}
n \log n - n + 1 \le M(n) = n \ceil{\log n} - 2^{\ceil{\log n}} + 1 \le n \log
n + n + \BigO{\log n},
\end{displaymath}
while
\begin{displaymath}
\ITLB \simeq n \log n - 1.443 n + \frac{1}{2} \log n + 1.326,
\end{displaymath}
which leaves us with a \BigO{n} gap between the two bounds.

However, it is possible to do better. The Ford-Johnson
algorithm~\cite{ford:1959,hwang:1969,knuth1998art} is the
best-known sorting algorithm for this purpose. The bounds achieved by the
Ford-Johnson algorithm are
\begin{displaymath}
n \log n - 1.415 n + \frac{1}{2} \log n + \BigO{1} \le F(n) \le n \log n - 1.329 n +
\frac{1}{2} \log n + \BigO{1},
\end{displaymath}
where
\begin{displaymath}
F(n) = n \log n - n ( 1 + f + 2^{1-f} - \log 3) + \frac{1}{2} \log n + \BigO{1}
\end{displaymath}
and \(n = \left(\frac{4}{3}\right) 2^{k+f}\) with \(k\) an integer and \(f\) a real
number in the range \(0 < f < 1\).

\citet*{manacher:1979} proved that this algorithm is not optimal, \ie that
there exists another sorting algorithm beating the Ford-Johnson algorithm in
terms of number of comparisons for some inputs. The question of the existence
of an optimal algorithm remains open.
