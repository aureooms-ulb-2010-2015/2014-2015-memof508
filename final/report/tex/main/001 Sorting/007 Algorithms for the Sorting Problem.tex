\section{Algorithms for the Sorting Problem}
\label{tree:sorting:alg}

We will now detail three algorithms that can be used to solve the sorting
problem efficiently. According to our model, remember \ref{tree:sorting:model},
we only consider comparison-based sorting algorithms.

\nb{In the following paragraphs and subsections we will use the words minimum,
smaller
and smallest as well as the symbol \(\le\). If one replaces minimum by maximum,
smaller by larger, smallest by largest and \(\le\) by \(\ge\) in those pargraphs and subsections,
their content remains valid.}

Before we start, let us make a quick observation. The sorting problem can
easily be solved using at most \(\binom{n}{2} = \BigO{n^2}\) queries. A first
way to achieve this bound is by iteratively computing and removing the minimum
element from the input set. A second way is to iteratively
extend a sorted sequence by insertion of an element taken (and removed) from
the input set. Selection sort and Bubble sort are two algorithms based on the
first method. Insertion sort is an algortihm based on the second method.
Because the complexity of these algorithms does not meet the ITLB, we will need
to think about more clever methods\footnote{Although it is possible to obtain
an \BigO{n \log n} comparisons algorithm by replacing the search method of the
Insertion sort algorithm by a binary search, this approach does not lead directly
to a practical algorithm. Indeed, in the machine-world, one must also take
into account other types of operations. In the case of the Insertion Sort
algorithm, no matter the search method, the number of memory read and write
operations is quadratic on average.}.

\subsection*{\quicksort}

The first efficient algorithm we will explain is \quicksort \cite{hoare:1962}.
The idea behind the \quicksort algorithm is to divide the input set \(\S\),
\(\card{\S} = n\), into two disjoint subsets \(\L\) and \(\U\) that can be
sorted independently from one another. To enforce this independence condition,
the first part of the algorithm consists in computing \(\L\) and \(\U\) such
that \(l \le u \Forall l \in \L, u \in U\). Once \(\L\) and \(\U\) are known,
the algorithm sorts these two sets recursively. A rigorous definition of the
algorithm is the following:

\begin{algorithm}
\item[1.] If \(n < 2\) then \(\S\) is sorted and the algorithm stops.
\item[2.] Choose a pivot element \(p \in \S\) and compare it with all other
elements of \(\S\) by asking \(n - 1\) queries to the oracle.
\item[3.] Partition \(\T = \S \setminus \enum{p}\) into two disjoint subsets
\(\L = \enum{t \in \T \st t \le p}\) and \(\U = \T \setminus \L\).
\item[4.] Sort \(\L\).
\item[5.] Sort \(\U\).
\end{algorithm}

The \quicksort algorithm is a divide-and-conquer algorithm: it splits a
bigger problem into several smaller problems whose solutions can be combined
to build the solution of the bigger problem. Ideally, \(p\) would be chosen to
be the median element of \(\S\) so that \(\card{\L} =
\ceil{\frac{n-1}{2}}\) and \(\card{\U} =
\floor{\frac{n-1}{2}}\), because then the recursion depth of the
algorithm would be bounded by \(\floor{\log \card{S}}\) resulting in a worst
case complexity of \BigO{n \log n}. However, if \(p\) can be any element of
\(\S\) then in the worst case \(p\) is the minimum (or maximum) element of
\(\S\) and \(\L\) (or \(\U\)) is empty. In this case, \quicksort behaves like
the Selection sort algorithm and its worst case complexity degrades to
\BigO{n^2}. It is possible \cite{blum:1973} to compute a good pivot or even the
median element in \BigO{n} comparisons. However, it was shown \cite{hoare:1962} that
selecting the pivot uniformly at random leads to an average case complexity of
\(1.39~n \log n\) comparisons, and for real-world computers this approach tends
to be more efficient than the safe one.

\subsection*{\mergesort}

A second efficient algorithm is \mergesort
\cite{goldstine:1948,leiserson:2001}. Like \quicksort, \mergesort is a
divide-and-conquer algorithm. While in \quicksort one makes the comparisons from
the partition step before solving the two subproblems, in \mergesort we will
first solve two subproblems and then recombine them with a merge step that
requires \BigO{n} comparisons:

\begin{algorithm}
\item[1.] If \(n < 2\) then \(\S\) is sorted and the algorithm stops.
\item[2.] Arbitrarily partition \(\S\) into two disjoint subsets \(\L\) and
\(\U\) such that \(\card{\L} = \ceil{\frac{n-1}{2}}\) and \(\card{\U} =
\floor{\frac{n-1}{2}}\).
\item[3.] Sort \(\L\).
\item[4.] Sort \(\U\).
\item[5.] Merge \(\L\) and \(\U\) by iteratively removing the smallest element
of \(\S\).
\end{algorithm}

Note that in step \step{5} there are at most \(n - 1\) iterations. Since \(\L\)
and \(\U\) are sorted, the smallest element of \(\S\) can be computed as the
smallest element between the smallest element of \(\L\) and the smallest
element of \(\U\). Hence, each iteration requires a single comparison to be
made.

Unlike the previous algorithm, we will always obtain a balanced partition of
\(\S\) out of the partitioning step. Hence, the worst-case complexity of this
algorithm is \BigO{n \log n}. An other way to obtain this complexity measure
is to solve the following recurrence relation:

\begin{displaymath}
T(n) = T(\ceil{\frac{n}{2}}) + T(\floor{\frac{n}{2}}) + n - 1.
\end{displaymath}

\subsection*{Heapsort}

The last algorithm we will explain is the Heapsort algorithm
\cite{williams:1964}. A heap is a data structure that allows one to
efficiently lookup (\BigO{1}) or remove (\BigO{\log n}) the smallest element
from a set \(\S\). A heap can be seen as a complete \(d\)-ary tree where a node
is smaller than its children. The last condition is called the heap property.
Constructing a heap for \(\S\) can be done in \BigO{n} comparisons.  Let us
explain how. There exist two main procedures to update the contents of a heap.
The first one is \emph{siftdown} which consists in \emph{sifting} a node down
the tree until it is smaller or equal to all of its children. Below are the
steps of the \emph{siftdown} algorithm with input node \(T\):

\begin{algorithm}
\item[1.] If \(T\) is a leaf of the tree the algorithm stops.
\item[2.] Compute \(C = \min \text{children}(T)\).
\item[3.] If \(T \le C\) the algorithm stops.
\item[4.] Otherwise, swap \(C\) and \(T\).
\item[5.] Sift down \(T\) again.
\end{algorithm}

The second one is \emph{siftup}. This operation is similar to \emph{siftdown}.
We \emph{sift} a node up the tree until it is larger ore equal to its parent.
Below are the steps of the algorithm:

\begin{algorithm}
\item[1.] If \(T\) is the root of the tree the algorithm stops.
\item[2.] Compute \(P = \text{parent}(T)\).
\item[3.] If \(T \ge P\) the algorithm stops.
\item[4.] Otherwise, swap \(P\) and \(T\).
\item[5.] Sift up \(T\) again.
\end{algorithm}

For example, to remove the minimum from a binary heap one replaces the root
node with the rightmost leaf of the complete binary tree and then sifts down
the new root. To add a new node, one attaches this node to the tree to the
right of the rightmost leaf (or as the leftmost node of a new level
of the tree if the last level is complete) and then sifts up this new node.
Since a complete tree is balanced, sifting down a node in a binary heap uses
at most \(2 \ceil{\log n}\) comparisons while sifting up a node uses at most
\(\ceil{\log n}\)
comparisons. However, it is possible to build a heap using \SmallO{n \log n}
comparisons. Assume we have a complete tree that does not respect the heap
property. This tree has \(\ceil{\frac{n}{2}}\) leaves, at most
\(\ceil{\frac{n}{4}}\) nodes of height \(1\), at most \(\ceil{\frac{n}{8}}\)
nodes of height \(2\), \dots, \ie at most \(\ceil{\frac{n}{2^{1+h}}}\) nodes
of height \(h\).
One applies \emph{siftdown} to each node of the tree right-to-left,
bottom-to-top, starting from the rightmost leaf and ending at the root. The
total number of comparisons is

\begin{displaymath}
T(n) \le 2 \cdot ( 0 \cdot \ceil{\frac{n}{2}} + 1 \cdot \ceil{\frac{n}{4}} + 2 \cdot
\ceil{\frac{n}{8}} + 3 \cdot \ceil{\frac{n}{16}} + \cdots ) = \BigO{n}.
\end{displaymath}

To sort \(\S\) using a heap, one builds a heap with \BigO{n} comparisons and
then iteratively extracts the minimum of this heap and restores the heap
property with \(2\ceil{\log (n-1)}\) comparisons. The maximum number of
comparisons involved in these extraction steps is

\begin{displaymath}
T(n) = 2 \sum_{i=2}^{n-1} \ceil{\log i} \le 2 ( \log (n-1)! + n - 2 ),
\end{displaymath}

which is approximately twice as much comparisons than what can be found by
solving the recurrence relation for the \mergesort algorithm (see
\citet*{OEIS:A001855}). Note that in the Heapsort algorithm we did not use the
\emph{siftup} operation. There exists \cite{wegener:1993} a variation of the
extracting step leading to an algorithm called bottom-up Heapsort that uses at
most \(1.5~n \log n + \BigO{n}\) comparisons.

\subsection*{Other Algorithms}

There exists a lot of other interesting ideas to solve the sorting problem. For
example, Shellsort \cite{shell:1959}, Smoothsort \cite{dijkstra:1982},
Introsort \cite{musser:1997} and Timsort \cite{martelli:2006}.

Considering practical use of sorting algorithms, \quicksort is one of the most
efficient algorithms on real-world computers due to its simplicity and cache
locality of its operations, even though \quicksort has a much higher
multiplicative factor than \mergesort. Nowadays, standard libraries of the most
popular programming languages (\emph{Java} (up to \emph{Java
6}),\emph{C},\emph{C++}) use their own version of the \quicksort algorithm as
their sorting procedure. An exception is \emph{Python} which uses the Timsort
algorithm since 2002. Note also that up to \emph{Java 6}, \emph{Java} used a
\mergesort algorithm for sorting non-primitive types. The reason behind this
choice is that objects in \emph{Java} can be different while having the same
comparison ``key''. Since the \emph{Java} \mergesort algorithm is implemented as
a stable sorting procedure, it offers the users the guarantee that the sorting
algorithm will not reverse the order of equal-``key'' objects. The classical
\quicksort algorithm is not used in this case because it is not stable.

Recently, there has been a lot of interest in a dual-pivot \quicksort
implementation by \citet*{yaroslavskiy:2009}. It is not the first time this
approach is considered. Previous attempts \cite{sedgewick:1980} had concluded
that a dual-pivot \quicksort was worse than the classical one in practice.
Analysis of the version of \citet*{yaroslavskiy:2009} by \citet*{wild:2012}
reveals that this new version uses \(1.32~n \log n - 2.46n + \BigO{\log n}\)
comparisons.  This is less than the classical \quicksort algorithm which uses
\(1.39~n \log n
- 1.51n + \BigO{\log n}\) comparisons. However, in practice one must also take into account
  other kinds of operations. For example, the number of element swaps in the
dual-pivot \quicksort of \citeauthor{yaroslavskiy:2009} is twice the number of
element swaps of the classic \quicksort by \citeauthor{hoare:1962}. This
algorithm is now used as the sorting procedure of arrays of primitive type for
the \emph{Java} language (since \emph{Java 7}). Although it performs more
swaps \cite{wild:2012}, and uses more Java Bytecodes \cite{wild:2013}, the
dual-pivot version of \citet*{yaroslavskiy:2009} outperformed the previous
\quicksort implementation experimentally. Note that the stable \mergesort
algorithm of \emph{Java 7} was changed for a Timsort implementation, which is
also stable. One of the subtleties of the Timsort algorithm is that it tries
to exploit already sorted subsequences of the input. On a sorted or reverse
sorted input the Timsort algorithms runs in \BigO{n} time.

A remark concerning Shellsort. The average time complexity of Shellsort is not
known for the moment. Indeed, there are multiple ways of implementing this
algorithm because of its definition. The Shellsort algorithm is configurable
with a sequence of integers called a \emph{gap sequence}. This gap sequence
parameter leads to different time complexities. For example, depending on the
implementation, one can achieve a complexity of either $\BigO{n \log^2 n}$ or
$\BigO{n^{\frac{3}{2}}}$. There are some practical variants of this algorithm
for which the time complexity is not known. An open problem is to find whether
there exists a gap sequence that makes the algorithm run in $\BigO{n \log
n}$.

In addition to classical sorting algorithms, we may also mention that any data
structure allowing to retrieve inserted elements in an ordered fashion
(according to some total order) with insertion and retrieve operations in
$\BigO{\log n}$ time complexity can be used to build an algorithm whose time
complexity is $\BigO{n \log n}$. Examples are,

\begin{itemize}
\item Binary tree sort which uses a \emph{binary tree};
\item Tournament sort which uses a \emph{priority queue}.
\end{itemize}

For example implementations of those data structures see
\citet*{sleator:1985} for the binary tree, and
\citet*{leiserson:2001} for the priority queue.


\subsection*{Is the Sorting Problem a Solved Problem?}

Although there has been an enormous amount of research directed towards sorting
algorithms in the field of Computer Science, there still exists a linear term
gap between the most efficient algorithms and the \concept{ITLB} of the
sorting problem.

For example, \mergesort is one of the algorithms whose complexity comes closest
to the \concept{ITLB}, \ie
\begin{displaymath}
n \log n - n + 1 \le T(n) = n \ceil{\log n} - 2^{\ceil{\log n}} + 1 \le n \log
n + n + \BigO{\log n},
\end{displaymath}
while
\begin{displaymath}
\text{ITLB} \simeq n \log n - 1.443 n + \frac{1}{2} \log n + 1.326,
\end{displaymath}
which leaves us with a \BigO{n} gap in the best case.
